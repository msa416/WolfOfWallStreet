{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures and Algorithms\n",
    "## Group 4\n",
    "## Final Project\n",
    "## Stock Market Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages and data as pandas data frame. Define graphs and directed graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "##### Read company names into a dictionary\n",
    "#def readNamesIntoDict():\n",
    "#    d = dict()\n",
    "#    input_file = csv.DictReader(open(\"SP_500_firms.csv\"))\n",
    "#    for row in input_file:\n",
    "#        #print(row)\n",
    "#        d[row['Symbol']] = [row['Name'],row['Sector']]\n",
    "#   return d\n",
    "#\n",
    "#namesDict = readNamesIntoDict()\n",
    "#\n",
    "#compNames = namesDict.keys()\n",
    "#\n",
    "#\n",
    "###### Prices into standarad Python data structures\n",
    "#\n",
    "## Read prices into dictionary of lists\n",
    "##\n",
    "#def readPricesIntoDict():\n",
    "#    input_file = csv.DictReader(open('SP_500_close_2015.csv', 'r')) \n",
    "#    d = dict()\n",
    "#    for row in input_file:\n",
    "#        for column, value in row.items():\n",
    "#            d.setdefault(column, []).append(value)\n",
    "#    return d\n",
    "\n",
    "\n",
    "class Digraph():\n",
    "    #edges is a dict mapping each node to a dictionary of edges starting from it\n",
    "    def __init__(self,filename = None):\n",
    "        self.edges = {}\n",
    "        self.numEdges = 0\n",
    "    \n",
    "    def addNode(self,node):\n",
    "        # add a vertex to graph (based on key value)       \n",
    "        self.edges[node] = set()\n",
    "\n",
    "    def addEdge(self,src,dest,weight):\n",
    "        # add the (v,w) edge, making sure the two nodes exist\n",
    "        if not self.hasNode(src): \n",
    "            self.addNode(src)\n",
    "            self.edges[src] = {}\n",
    "        if not self.hasNode(dest): \n",
    "            self.addNode(dest)\n",
    "            self.edges[dest] = {}\n",
    "        if not self.hasEdge(src, dest):\n",
    "            self.numEdges += 1\n",
    "            self.edges[src][dest] = weight\n",
    "  \n",
    "    def childrenOf(self, v):\n",
    "        # Returns a node's children\n",
    "        return self.edges[v].items()\n",
    "        \n",
    "    def hasNode(self, v):\n",
    "        return v in self.edges\n",
    "        \n",
    "    def hasEdge(self, v, w):\n",
    "        return w in self.edges[v]\n",
    "\n",
    "    def listEdges(self):\n",
    "        ll = []\n",
    "        for src,values in self.edges.items():\n",
    "            for dest,weight in values.items():\n",
    "                ll.append([src,dest,weight])\n",
    "        return ll\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = ''\n",
    "        for src in self.edges:\n",
    "            for dest,weight in self.edges[src].items():\n",
    "                result = result + src + '->'\\\n",
    "                         + dest + ', length ' + str(weight) + '\\n'\n",
    "        return result[:-1] # omit final newline\n",
    "\n",
    "class Graph(Digraph):\n",
    "    \"\"\" Undirected graph: two one-way edges for every edge\n",
    "        \"\"\"\n",
    "    def addEdge(self, src, dest,weight):\n",
    "        Digraph.addEdge(self, src, dest,weight)\n",
    "        Digraph.addEdge(self, dest, src,weight)\n",
    "        \n",
    "        \n",
    "        \n",
    "filename = 'SP_500_close_2015.csv'\n",
    "priceData = pd.read_csv(filename,index_col = 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculating Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for company in priceData:\n",
    "\tcompanyData = priceData[company]\n",
    "\tpct_change = companyData[1:]/companyData[:-1]\n",
    "\t#print(pct_change)\n",
    "\n",
    "percent_change = priceData.pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def covariance(x,y):\n",
    "    mean_x=np.mean(x)\n",
    "    mean_y=np.mean(y)\n",
    "    return np.mean((x-mean_x)*(y-mean_y))\n",
    "\n",
    "\n",
    "def correlation(x,y):\n",
    "    return covariance(x,y)/(np.std(x)*np.std(y))\n",
    "\n",
    "\n",
    "correlation_coefficient = {}\n",
    "    \n",
    "for company1 in percent_change:\n",
    "    correlation_coefficient[company1]={}\n",
    "    for company2 in percent_change:\n",
    "        x=percent_change[company1]        \n",
    "        y=percent_change[company2]\n",
    "        if company1==company2:\n",
    "            correlation_coefficient[company1][company2]=1\n",
    "        else:\n",
    "            correlation_coefficient[company1][company2]=correlation(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sortedweights [('c', 'd', 0.95), ('a', 'b', 0.8), ('b', 'c', 0.68), ('a', 'c', 0.63), ('b', 'd', 0.32), ('a', 'd', 0.05)]\n",
      "True d\n",
      "{'d': 'd', 'c': 'd', 'a': 'a', 'b': 'b'}\n",
      "{'d': True, 'c': False, 'a': True, 'b': True}\n",
      "{'d': False, 'c': True, 'a': True, 'b': True}\n",
      "True b\n",
      "{'d': 'd', 'c': 'd', 'a': 'b', 'b': 'b'}\n",
      "{'d': True, 'c': False, 'a': False, 'b': True}\n",
      "{'d': False, 'c': True, 'a': True, 'b': False}\n",
      "False c\n",
      "{'d': 'b', 'c': 'd', 'a': 'b', 'b': 'b'}\n",
      "{'d': False, 'c': False, 'a': False, 'b': True}\n",
      "{'d': False, 'c': True, 'a': True, 'b': False}\n"
     ]
    }
   ],
   "source": [
    "input = [('a','b',.8),('a','c',.63),('c','d',.95),('b','d',.32),('b','c',.68),('a','d',.05)]\n",
    "\n",
    "def mergeSort(array):\n",
    "\tif len(array) > 1:\n",
    "\t\tmid = len(array) //2\n",
    "\t\tleft = array[:mid]\n",
    "\t\tright = array [mid:]\n",
    "\t\tmergeSort(left)\n",
    "\t\tmergeSort(right)\n",
    "\n",
    "\t\ti = 0\n",
    "\t\tj = 0\n",
    "\t\tk = 0\n",
    "\t\twhile i < len(left) and j < len(right):\n",
    "\t\t\tif left[i][2] > right[j][2]:\n",
    "\t\t\t\tarray[k] = left[i]\n",
    "\t\t\t\ti = i + 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tarray[k] = right[j]\n",
    "\t\t\t\tj = j + 1\n",
    "\t\t\tk = k+1\n",
    "\t\twhile i < len(left):\n",
    "\t\t\tarray[k] = left[i]\n",
    "\t\t\ti += 1\n",
    "\t\t\tk += 1\n",
    "\t\twhile j < len(right):\n",
    "\t\t\tarray[k] = right[j]\n",
    "\t\t\tj += 1\n",
    "\t\t\tk += 1\n",
    "\treturn(array)\n",
    "sortedWeights = mergeSort(input)\n",
    "print(\"sortedweights\",sortedWeights)\n",
    "\n",
    "graph = Digraph()\n",
    "for x in input:\n",
    "\tgraph.addEdge(x[0],x[1],x[2])\n",
    "#print(graph)\n",
    "\n",
    "\n",
    "nodePointers = {src:src for src in graph.edges}\n",
    "nodeBottom = {src:True for src in graph.edges}\n",
    "nodeTop = {src:True for src in graph.edges}\n",
    "\n",
    "def findbottom(node):\n",
    "    source = node\n",
    "    destination = nodePointers[source]\n",
    "    if nodeBottom[destination]:\n",
    "        return destination\n",
    "    else:\n",
    "        return findbottom(destination)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for k in sortedWeights:\n",
    "    if counter < 4:\n",
    "        if nodePointers[k[0]] == k[0]:\n",
    "            print (nodeBottom[k[1]], k[1])\n",
    "            if nodeBottom[k[1]]:\n",
    "                nodePointers[k[0]] = k[1]\n",
    "                nodeBottom[k[0]] = False\n",
    "                nodeTop[k[1]] = False\n",
    "            else:\n",
    "                bottom = findbottom(k[1])\n",
    "                nodePointers[bottom] = k[0]\n",
    "                nodeBottom[bottom] = False\n",
    "\n",
    "\n",
    "\n",
    "            print(nodePointers)\n",
    "            print(nodeBottom)\n",
    "            print(nodeTop)\n",
    "            \n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Alternative Clustering Methods\n",
    "\n",
    "### Using correlations\n",
    "Suppose we are solely interested in clustering the stocks based on correlation. Then we might rely on other methods.\n",
    "\n",
    "#### Agglomerative heirachical clustering\n",
    "An example that is very similar to the one we have done is agglomerative clustering. The main difference is that the algorithm does not terminate after $k$ steps. Instead, it keeps going until all clusters are merged. At each step, we record the \"distance\" between clusters when merging. When the algorithm terminates, we need only examine the \"distances\" between the clusters and choose a maximimum \"distance\", then break the links between all clusters that were at least that far apart before merging.\n",
    "\n",
    "The advantage of this method is that we do not need to determine k before we begin the algorithm, we need only run the algorithm once and decide what $k$ should be after. Alternatively, plotting a dendogram would give us a visual aid to determine our cut off distance. Furthermore, by changing the way we determine distance between clusters, we could achieve much tighter clusters.\n",
    "\n",
    "The disadvantage is that there is a need to decide how to calculate \"distance\" between clusters. The distance between any two nodes is straightforward, the correlation between the two nodes. Now consider that we have two clusters of size $n$ and $m$. In this case how do we define distance? Do we use single linkage, the shortest distance between any two nodes from both clusters? Or complete linkage, the farthest distance?\n",
    "\n",
    "In our case, we want to cluster stocks such that they are all closely related. It would make sense to use average linkage, the average correlation between all pairwise stocks. This helps to enforce clusters that are all close to each other, with no node in a cluster being too far away from the other. \n",
    "\n",
    "Another major disadvantage to using heirachical clustering is the need to update our distances at every iteration. Adding on the fact that calculating the distance between two clusters is O(nm), the time complexity of carrying out this operation would be much greater. \n",
    "\n",
    "\n",
    "#### $k$ medoids clustering\n",
    "\n",
    "The basic idea of $k$ medoids clustering in this context is to pick a pre-determined $k$ nodes as \"centers\" , put them into $k$ different clusters, then assign every other node to the nearest center. In a $k$-means algorithm, in the next iteration, we would determine a \"center\". This is done by finding the means of each cluster. However, this would require us to provide co-ordinates, which is something we do not have for this problem. Instead, the $k$ medoids algorithm simply picks the node that is closest to the center to be the new center. For every node, we calculate the sum of its distance ($1$-correlation) to every other member of its cluster. Then for each cluster, pick the node with the smallest sum of distance ot be the new medoid, then carry out the next iteration.\n",
    "\n",
    "1. Randomly assign $k$ nodes as medoids of $k$ clusters\n",
    "2. Assign every other node to cluster of its closest medoid\n",
    "3. For each cluster:\n",
    "  1. For each node in cluster:\n",
    "    1. Calculate sum of distance between node and all other nodes in cluster\n",
    "  2. Set node with smallest sum of distance as new medoid\n",
    "4. Repeat steps 2-3\n",
    "\n",
    "Calculating the sum of distance for every node in the graph is an expensive operation that runs in O($n^2$) time, where $n$ is the maximum number of nodes in a cluster. Instead of calculating all sum of distance, we can instead use a greedy approach, step 3-1 then becomes:\n",
    "\n",
    "1. For each node in cluster:\n",
    "  1. Calculate sum of distance\n",
    "  2. If new sum of distance < old sum of distance:\n",
    "    1. Set node as new medoid\n",
    "    2. break\n",
    "      \n",
    "The main advantage of this method over the others is its ability to re-allocate nodes into different clusters. For the given algorithm and heirachical clustering, once a node is in a cluster, it will never be re-allocated. With $k$ medoids however, the algorithm will constantly re-assign nodes into the most appropriate cluster.\n",
    "\n",
    "The disadvantage is the need to pre-specify the number of clusters. Before running any algorithms, it is hard to determine how many clusters there should be.\n",
    "\n",
    "To counter this problem, we can consider a hybrid of both solutions. First, we do a heirachical cluster to determine the number of clusters, $k$. Using this result, we then run the $k$-medoids algorithm.\n",
    "\n",
    "### Using all data\n",
    "\n",
    "#### Artificial Neural Network\n",
    "Something something\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
